[2024-11-13 19:55:20 root] (main_block_ap.py 123): INFO Namespace(model=None, cache_dir='./cache', output_dir='./output/inference_results/', save_quant_dir=None, real_quant=False, resume_quant='./output/pre_quantized_models/Llama-2-7b-EfficientQAT-w2g64', calib_dataset='redpajama', train_size=4096, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='', eval_batch_size=16, wbits=2, group_size=64, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net='Llama-2', max_memory='70GiB', early_stop=0, off_load_to_disk=False)
[2024-11-13 19:55:20 root] (main_block_ap.py 128): INFO Using device: cuda
[2024-11-13 19:55:23 root] (main_block_ap.py 167): INFO Memory footprint after loading quantized model: 2.28GiB
[2024-11-13 19:55:23 datasets.load] (load.py 1600): WARNING Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
[2024-11-13 19:55:23 datasets.packaged_modules.cache.cache] (cache.py 72): WARNING Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/biswajit/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Nov 13 14:24:07 2024).
